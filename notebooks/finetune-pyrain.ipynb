{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6eacb994-ea21-4748-8750-722397a931ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fcfee9-9ab2-436d-a96a-7f6fbef4162a",
   "metadata": {},
   "source": [
    "# Finetuning Pyrain\n",
    "\n",
    "This notebook will demonstrate how to finetune climaX on the RainBench preciptation data and code provided by [Pyrain](https://github.com/FrontierDevelopmentLab/PyRain/tree/master). This notebook is based on the [Pytorch Lightning](https://pytorch-lightning.readthedocs.io/en/latest/) framework and can be adapted to other datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c27939e-9802-4f6f-86a3-c67a2d8aed57",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Environment\n",
    "Make sure the environment is set up for training [climaX](https://microsoft.github.io/ClimaX/install/). Also, install the following packages required for this notebook in the same environment:\n",
    "\n",
    "```bash\n",
    "pip install dill\n",
    "pip install deepspeed\n",
    "# i think there were more, but i forgot\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3040a11-f58a-4bad-85fe-64e5cc772476",
   "metadata": {},
   "source": [
    "### Data\n",
    "The precipitation data provided by Pyrain can be downloaded [here](https://console.cloud.google.com/storage/browser/aaai_release).\n",
    "\n",
    "#### Input\n",
    "For this notebook, we will use RainBench data from both Simsat and ERA5. Refer to the [Rainbench](https://arxiv.org/abs/2012.09670) paper for the complete list of variables. The input is composed of time series over a 12 hour period, sampled every 6 hours by default.\n",
    "\n",
    "![input.png](images/input.png)\n",
    "\n",
    "The shape is $T \\times V \\times H \\times W$, where $T$ is the number of input time steps, $V$ is the number of variables, and $H$, $W$ the spatial resolution (32 x 64 for 5.625Â°).\n",
    "\n",
    "#### Output\n",
    "The network will be trained to predict the precipitation at several lead times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2b7133",
   "metadata": {},
   "source": [
    "### Configs\n",
    "\n",
    "We will use a hparams dictionary to store all the hyperparameters for initializing the dataloaders and training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "744179d7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'seed': 2020,\n",
    "    'sources': 'simsat_era', # options: 'simsat_era', 'simsat', 'era'\n",
    "    'imerg': False, # options: True (predict IMERG), False (predict ERA5)\n",
    "    'grid': 5.625, \n",
    "    'sample_time_window': 12, \n",
    "    'sample_freq': 6,\n",
    "    'forecast_time_window': 120,\n",
    "    'forecast_freq': 24,\n",
    "    'inc_time': True,\n",
    "    'data_paths': [\n",
    "        '/localhome/data/datasets/rainbench/era5625_aaai/era5625_aaai-era5625_aaai.dill', \n",
    "        '/localhome/data/datasets/rainbench/imerg5625/imerg5625-imerg5625.dill', \n",
    "        '/localhome/data/datasets/rainbench/simsat5625/simsat5625-simsat5625.dill'\n",
    "    ],\n",
    "    'norm_path': 'pyrain/normalize.json',\n",
    "    'log_path': '/localhome/data/ckpts/seongbin/rainbench/',\n",
    "    'gpus': 1,\n",
    "    'use_amp': True,\n",
    "    'batch_size': 2,\n",
    "    'lr': 5e-05,\n",
    "    'num_workers': 8,\n",
    "    'ckpt': None,\n",
    "    'load': 'https://huggingface.co/tungnd/climax/resolve/main/5.625deg.ckpt',\n",
    "    # 'strategy': 'deepspeed_stage_2', # Deepspeed not available in interactive environments\n",
    "    'strategy': None,\n",
    "    'acc_grad': 1,\n",
    "    'version': 'pyrain-finetune-template',\n",
    "    'plot': False,\n",
    "    'beta_1': 0.9,\n",
    "    'beta_2': 0.999,\n",
    "    'weight_decay': 1e-05,\n",
    "    'warmup_epochs': 60,\n",
    "    'max_epochs': 100,\n",
    "    'warmup_start_lr': 1e-08,\n",
    "    'eta_min': 0.00000001\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abc35f2",
   "metadata": {},
   "source": [
    "## Datamodule\n",
    "\n",
    "We will first define a datamodule that will load the data and prepare it for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "077cbafc-91e4-4772-889b-6b4e918aca01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seongbin/miniconda3/envs/climaX/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-23 19:25:32,459] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import LightningDataModule\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from pyrain.dataset import RainbenchDataset\n",
    "from pyrain.collect_data import write_data_config, read_normalization_stats\n",
    "from pyrain.utils import get_local_shift, collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d3e38df",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class RainbenchDataModule(LightningDataModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.data_dir, self.partition_conf, self.sample_conf = write_data_config(hparams)\n",
    "        self.normalizer = read_normalization_stats(hparams['norm_path'])\n",
    "\n",
    "        self.train_dataset = RainbenchDataset(\n",
    "            datapath=self.data_dir,\n",
    "            partition_conf=self.partition_conf,\n",
    "            partition_type=\"range\",\n",
    "            partition_selected=\"train\",\n",
    "            sample_conf=self.sample_conf)\n",
    "\n",
    "        self.val_dataset = RainbenchDataset(\n",
    "            datapath=self.data_dir,\n",
    "            partition_conf=self.partition_conf,\n",
    "            partition_type=\"range\",\n",
    "            partition_selected=\"valid\",\n",
    "            sample_conf=self.sample_conf)\n",
    "\n",
    "        self.test_dataset = RainbenchDataset(\n",
    "            datapath=self.data_dir,\n",
    "            partition_conf=self.partition_conf,\n",
    "            partition_type=\"range\",\n",
    "            partition_selected=\"test\",\n",
    "            sample_conf=self.sample_conf)\n",
    "\n",
    "        time_shift = None\n",
    "        if hparams['inc_time']:\n",
    "            time_shift = get_local_shift(hparams['grid'], self.train_dataset.dataset)\n",
    "        self.collate = lambda x: collate_fn(x, hparams, self.normalizer, time_shift)\n",
    "\n",
    "    \n",
    "\n",
    "    def get_lat2d(self, grid):\n",
    "        if grid == 5.625:\n",
    "            lat2d = self.val_dataset.dataset['era5625/lat2d']\n",
    "        else:\n",
    "            lat = np.linspace(-89.296875, 89.296875, 128)\n",
    "            lat2d = np.expand_dims(lat, axis=1).repeat(256, 1)\n",
    "        return lat2d\n",
    "            \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=hparams['batch_size'], num_workers=hparams['num_workers'], collate_fn=self.collate, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=hparams['batch_size'], num_workers=hparams['num_workers'], collate_fn=self.collate, shuffle=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=hparams['batch_size'], num_workers=hparams['num_workers'], collate_fn=self.collate, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222ca99e",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "Since the [input](#Input) and [output](#Output) are different from the original climaX task, we will need to modify the model architecture. We will use the same encoder and decoder (no freezing), but change the head to predict just one variable (precipitation) at a time. We also must aggregate multiple time steps into a single input and add a time embedding to the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ec5c8d1-1afd-4fc9-a7d9-ee1ecee30f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from climax.arch import ClimaX\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from climax.utils.pos_embed import get_1d_sincos_pos_embed_from_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2eaa8ffb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class ClimaXRainBench(ClimaX):\n",
    "    def __init__(\n",
    "        self,\n",
    "        default_vars,\n",
    "        out_vars,\n",
    "        img_size=[32, 64],\n",
    "        time_history=1,\n",
    "        patch_size=2,\n",
    "        embed_dim=1024,\n",
    "        depth=8,\n",
    "        decoder_depth=2,\n",
    "        num_heads=16,\n",
    "        mlp_ratio=4.0,\n",
    "        drop_path=0.1,\n",
    "        drop_rate=0.1,\n",
    "        parallel_patch_embed=False,\n",
    "        freeze_encoder=False,\n",
    "    ):\n",
    "        assert out_vars is not None\n",
    "\n",
    "        super().__init__(\n",
    "            default_vars,\n",
    "            img_size,\n",
    "            patch_size,\n",
    "            embed_dim,\n",
    "            depth,\n",
    "            decoder_depth,\n",
    "            num_heads,\n",
    "            mlp_ratio,\n",
    "            drop_path,\n",
    "            drop_rate,\n",
    "            parallel_patch_embed\n",
    "        )\n",
    "\n",
    "        self.out_vars = out_vars\n",
    "        self.time_history = time_history\n",
    "        self.freeze_encoder = freeze_encoder\n",
    "\n",
    "        # used to aggregate multiple timesteps in the input\n",
    "        self.time_pos_embed = nn.Parameter(torch.zeros(1, time_history, embed_dim), requires_grad=True)\n",
    "        self.time_agg = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "        self.time_query = nn.Parameter(torch.zeros(1, 1, embed_dim), requires_grad=True)\n",
    "\n",
    "        # initialize time embedding\n",
    "        time_pos_embed = get_1d_sincos_pos_embed_from_grid(self.time_pos_embed.shape[-1], np.arange(self.time_history))\n",
    "        self.time_pos_embed.data.copy_(torch.from_numpy(time_pos_embed).float().unsqueeze(0))\n",
    "\n",
    "        # overwrite ClimaX\n",
    "        # replace head for this task, since output is different\n",
    "        self.head = nn.ModuleList()\n",
    "        for _ in range(decoder_depth):\n",
    "            self.head.append(nn.Linear(embed_dim, embed_dim))\n",
    "            self.head.append(nn.GELU())\n",
    "        self.head.append(nn.Linear(embed_dim, patch_size**2))\n",
    "        self.head = nn.Sequential(*self.head)\n",
    "\n",
    "        if freeze_encoder:\n",
    "            for name, p in self.blocks.named_parameters():\n",
    "                name = name.lower()\n",
    "                # we do not freeze the norm layers, as suggested by https://arxiv.org/abs/2103.05247\n",
    "                if 'norm' in name:\n",
    "                    continue\n",
    "                else:\n",
    "                    p.requires_grad_(False)\n",
    "\n",
    "\n",
    "    def forward_encoder(self, x: torch.Tensor, lead_times: torch.Tensor, variables):\n",
    "        # x: `[B, T, V, H, W]` shape\n",
    "\n",
    "        if isinstance(variables, list):\n",
    "            variables = tuple(variables)\n",
    "        \n",
    "        b, t, _, _, _ = x.shape\n",
    "        x = x.flatten(0, 1)  # BxT, V, H, W\n",
    "        \n",
    "        # tokenize each variable separately\n",
    "        embeds = []\n",
    "        var_ids = self.get_var_ids(variables, x.device)\n",
    "\n",
    "        if self.parallel_patch_embed:\n",
    "            x = self.token_embeds(x, var_ids)  # BxT, V, L, D\n",
    "        else:\n",
    "            for i in range(len(var_ids)):\n",
    "                id = var_ids[i]\n",
    "                embeds.append(self.token_embeds[id](x[:, i : i + 1]))\n",
    "            x = torch.stack(embeds, dim=1)  # BxT, V, L, D\n",
    "\n",
    "        # add variable embedding\n",
    "        var_embed = self.get_var_emb(self.var_embed, variables)\n",
    "        x = x + var_embed.unsqueeze(2)  # BxT, V, L, D\n",
    "\n",
    "        # variable aggregation\n",
    "        x = self.aggregate_variables(x)  # BxT, L, D\n",
    "\n",
    "        # add pos embedding\n",
    "        x = x + self.pos_embed\n",
    "\n",
    "        # add time embedding\n",
    "        # time emb: 1, T, D\n",
    "        x = x.unflatten(0, sizes=(b, t)) # B, T, L, D\n",
    "        x = x + self.time_pos_embed.unsqueeze(2)\n",
    "\n",
    "        # add lead time embedding\n",
    "        lead_time_emb = self.lead_time_embed(lead_times.unsqueeze(-1)) # B, D\n",
    "        lead_time_emb = lead_time_emb.unsqueeze(1).unsqueeze(2)\n",
    "        x = x + lead_time_emb # B, T, L, D\n",
    "\n",
    "        x = x.flatten(0, 1)  # BxT, L, D\n",
    "\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        # apply Transformer blocks\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x) # BxT, L, D  \n",
    "        x = x.unflatten(0, sizes=(b, t)) # B, T, L, D\n",
    "\n",
    "        time_query = self.time_query.repeat_interleave(x.shape[0], dim=0)\n",
    "        # run time_agg for each L, so that the final output is B, L, D\n",
    "        agg_x = torch.empty(0, dtype=x.dtype).to(x.device)\n",
    "        for i in range(x.shape[2]):\n",
    "            agg_x_i, _ = self.time_agg(time_query, x[:, :, i, :], x[:, :, i, :])\n",
    "            agg_x = torch.cat((agg_x, agg_x_i), dim=1)\n",
    "\n",
    "        return agg_x    # B, L, D\n",
    "\n",
    "\n",
    "    def unpatchify(self, x: torch.Tensor, h=None, w=None):\n",
    "        \"\"\"\n",
    "        x: (B, L, patch_size**2)\n",
    "        return imgs: (B, 1, H, W)\n",
    "        \"\"\"\n",
    "        p = self.patch_size\n",
    "        c = 1\n",
    "        h = self.img_size[0] // p if h is None else h // p\n",
    "        w = self.img_size[1] // p if w is None else w // p\n",
    "        assert h * w == x.shape[1]\n",
    "\n",
    "        x = x.reshape(shape=(x.shape[0], h, w, p, p, c))\n",
    "        x = torch.einsum(\"nhwpqc->nchpwq\", x)\n",
    "        imgs = x.reshape(shape=(x.shape[0], c, h * p, w * p))\n",
    "        return imgs\n",
    "    \n",
    "\n",
    "    def forward(self, x, y, lead_times, variables, out_variables, metric, lat):\n",
    "        out_transformers = self.forward_encoder(x, lead_times, variables)  # B, L, D\n",
    "        \n",
    "        preds = self.head(out_transformers)  # B, L, p*p\n",
    "\n",
    "        preds = self.unpatchify(preds) # B, 1, H, W\n",
    "\n",
    "        if metric is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            loss = [m(preds, y, out_variables, lat) for m in metric]\n",
    "\n",
    "        return loss, preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e34b2aa",
   "metadata": {},
   "source": [
    "## Forecast Module\n",
    "\n",
    "We will now define the forecast module that will be used for training. This module will be initialized with the pretrained climaX model and will be finetuned on the precipitation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52ed208f-a049-4308-8a1b-ca66b63ae754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning import LightningModule, Trainer, loggers\n",
    "from climax.utils.lr_scheduler import LinearWarmupCosineAnnealingLR\n",
    "from climax.utils.metrics import (\n",
    "    mse,\n",
    "    lat_weighted_mse,\n",
    "    lat_weighted_nrmse, \n",
    "    lat_weighted_rmse,\n",
    ")\n",
    "from climax.utils.pos_embed import interpolate_pos_embed\n",
    "from typing import Any\n",
    "from pyrain.metrics import eval_loss, define_loss_fn, collect_outputs\n",
    "from deepspeed.ops import adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84a5e325",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class RainForecastModule(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        net,\n",
    "        lat2d,\n",
    "        normalizer,\n",
    "        pretrained_path: str = \"\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.categories = hparams['categories']\n",
    "        self.net = net\n",
    "        if len(pretrained_path) > 0:\n",
    "            self.load_pretrained_weights(pretrained_path)\n",
    "\n",
    "        self.lead_times = hparams['lead_times']\n",
    "        self.lat, self.lon = hparams['latlon']\n",
    "        self.test_step_outputs = []\n",
    "        self.val_step_outputs = []\n",
    "        self.version = hparams[\"version\"]\n",
    "        self.normalizer = normalizer\n",
    "        \n",
    "        self.weights_lat, self.loss = define_loss_fn(lat2d)\n",
    "        self.lat = lat2d[0][:,0]\n",
    "\n",
    "    def load_pretrained_weights(self, pretrained_path):\n",
    "        if pretrained_path.startswith(\"http\"):\n",
    "            checkpoint = torch.hub.load_state_dict_from_url(pretrained_path, map_location=torch.device(\"cpu\"))\n",
    "        else:\n",
    "            checkpoint = torch.load(pretrained_path, map_location=torch.device(\"cpu\"))\n",
    "\n",
    "        print(\"Loading pre-trained checkpoint from: %s\" % pretrained_path)\n",
    "        checkpoint_model = checkpoint[\"state_dict\"]\n",
    "        # interpolate positional embedding\n",
    "        interpolate_pos_embed(self.net, checkpoint_model, new_size=self.net.img_size)\n",
    "\n",
    "        state_dict = self.state_dict()\n",
    "        if self.net.parallel_patch_embed:\n",
    "            if \"token_embeds.proj_weights\" not in checkpoint_model.keys():\n",
    "                raise ValueError(\n",
    "                    \"Pretrained checkpoint does not have token_embeds.proj_weights for parallel processing. Please convert the checkpoints first or disable parallel patch_embed tokenization.\"\n",
    "                )\n",
    "\n",
    "        for k in list(checkpoint_model.keys()):\n",
    "            if \"channel\" in k:\n",
    "                checkpoint_model[k.replace(\"channel\", \"var\")] = checkpoint_model[k]\n",
    "                del checkpoint_model[k]\n",
    "\n",
    "            if \"head\" in k:\n",
    "                print(f\"Removing key {k} from pretrained checkpoint.\")\n",
    "                del checkpoint_model[k]\n",
    "\n",
    "        for k in list(checkpoint_model.keys()):\n",
    "            if k not in state_dict.keys() or checkpoint_model[k].shape != state_dict[k].shape:\n",
    "                print(f\"Removing key {k} from pretrained checkpoint\")\n",
    "                del checkpoint_model[k]\n",
    "\n",
    "        # load pre-trained model\n",
    "        msg = self.load_state_dict(checkpoint_model, strict=False)\n",
    "        print(msg)\n",
    "\n",
    "\n",
    "    def training_step(self, batch: Any, batch_idx: int):\n",
    "        x, y, lead_times = batch\n",
    "\n",
    "        loss_dict, p = self.net.forward(x, y, lead_times, self.categories['input'], self.categories['output'], [lat_weighted_mse], lat=self.lat)\n",
    "\n",
    "        loss_dict = loss_dict[0]\n",
    "        for var in loss_dict.keys():\n",
    "            self.log(\n",
    "                \"train/\" + var,\n",
    "                loss_dict[var],\n",
    "                on_step=True,\n",
    "                on_epoch=False,\n",
    "                prog_bar=True,\n",
    "            )\n",
    "        loss = loss_dict['loss']\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: Any, batch_idx: int):\n",
    "        x, y, lead_times = batch\n",
    "\n",
    "        _, pred = self.net.forward(\n",
    "            x,\n",
    "            y,\n",
    "            lead_times,\n",
    "            self.categories['input'],\n",
    "            self.categories['output'],\n",
    "            metric=None,\n",
    "            lat=self.lat,\n",
    "        )\n",
    "\n",
    "        results = eval_loss(pred, y, lead_times, self.loss, self.lead_times, phase='val', target_v=self.categories['output'][0], normalizer=self.normalizer)\n",
    "\n",
    "        self.val_step_outputs.append(results)\n",
    "        return results\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        node_loss = collect_outputs(self.val_step_outputs, False)\n",
    "        self.val_step_outputs.clear()  # free memory\n",
    "\n",
    "        if isinstance(node_loss, list):\n",
    "            node_loss = node_loss[0]\n",
    "    \n",
    "        all_losses = self.all_gather(node_loss)\n",
    "        mean_losses = {k: float(torch.mean(x)) for k, x in all_losses.items()}\n",
    "\n",
    "        # log mean losses\n",
    "        for var in mean_losses.keys():\n",
    "            self.log(\n",
    "                \"val/\" + var,\n",
    "                mean_losses[var],\n",
    "                sync_dist=True\n",
    "            )\n",
    "\n",
    "    def test_step(self, batch: Any, batch_idx: int):\n",
    "        x, y, lead_times = batch\n",
    "        _, pred = self.net.forward(\n",
    "            x,\n",
    "            y,\n",
    "            lead_times,\n",
    "            self.categories['input'],\n",
    "            self.categories['output'],\n",
    "            metric=None,\n",
    "            lat=self.lat,\n",
    "        )\n",
    "        results = eval_loss(pred, y, lead_times, self.loss, self.lead_times, phase='test', target_v=self.categories['output'][0], normalizer=self.normalizer)\n",
    "\n",
    "        self.test_step_outputs.append(results)\n",
    "        return results\n",
    "\n",
    "    def on_test_epoch_end(self) -> None:\n",
    "        node_loss = collect_outputs(self.test_step_outputs, False)\n",
    "        self.test_step_outputs.clear()  # free memory\n",
    "\n",
    "        if isinstance(node_loss, list):\n",
    "            node_loss = node_loss[0]\n",
    "    \n",
    "        all_losses = self.all_gather(node_loss)\n",
    "        mean_losses = {k: float(torch.mean(x)) for k, x in all_losses.items()}\n",
    "\n",
    "        # log mean losses\n",
    "        for var in mean_losses.keys():\n",
    "            self.log(\n",
    "                \"test/\" + var,\n",
    "                mean_losses[var],\n",
    "                sync_dist=True\n",
    "            )\n",
    "        \n",
    "        # Save evaluation results\n",
    "        results_path = Path(f'./results/{self.version}_results.json')\n",
    "        \n",
    "        with open(results_path, 'w') as fp:\n",
    "            json.dump(mean_losses, fp, indent=4)\n",
    "\n",
    "        fp.close()\n",
    "        \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        decay = []\n",
    "        no_decay = []\n",
    "        for name, m in self.named_parameters():\n",
    "            if \"var_embed\" in name or \"pos_embed\" in name or \"time_pos_embed\" in name:\n",
    "                no_decay.append(m)\n",
    "            else:\n",
    "                decay.append(m)\n",
    "\n",
    "        optimizer = adam.FusedAdam(\n",
    "            [\n",
    "                {\n",
    "                    \"params\": decay,\n",
    "                    \"lr\": hparams['lr'],\n",
    "                    \"betas\": (hparams['beta_1'], hparams['beta_2']),\n",
    "                    \"weight_decay\": hparams['weight_decay'],\n",
    "                },\n",
    "                {\n",
    "                    \"params\": no_decay,\n",
    "                    \"lr\": hparams['lr'],\n",
    "                    \"betas\": (hparams['beta_1'], hparams['beta_2']),\n",
    "                    \"weight_decay\": 0\n",
    "                },\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        lr_scheduler = LinearWarmupCosineAnnealingLR(\n",
    "            optimizer,\n",
    "            hparams['warmup_epochs'],\n",
    "            hparams['max_epochs'],\n",
    "            hparams['warmup_start_lr'],\n",
    "            hparams['eta_min'],\n",
    "        )\n",
    "        scheduler = {\"scheduler\": lr_scheduler, \"interval\": \"step\", \"frequency\": 1}\n",
    "\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c930eb65-fa19-4d60-9fd1-183a5a9e674e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define DataModule\n",
    "datamodule = RainbenchDataModule()\n",
    "lat2d = datamodule.get_lat2d(hparams['grid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d412094-818b-4f49-a501-0b926222242c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# define logger\n",
    "Path(hparams['log_path']).mkdir(parents=True, exist_ok=True)\n",
    "logger = loggers.TensorBoardLogger(hparams['log_path'], version=hparams['version'])\n",
    "logger.log_hyperparams(params=hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a1a14c2-7d99-47b8-9527-aad61281afa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define arch\n",
    "climax_var_order = [\"lsm\", \"orography\", \"lat2d\", \"t2m\", \"z-500\", \"z-850\", \"t-500\", \"t-850\", \"q-500\", \"q-850\"]\n",
    "categories = hparams['categories']\n",
    "sorted_vars = sorted(categories['input'], key=lambda x: climax_var_order.index(x) if x in climax_var_order else len(climax_var_order))\n",
    "net = ClimaXRainBench(\n",
    "    default_vars=sorted_vars,\n",
    "    out_vars=categories['output'],\n",
    "    time_history=3, # the number of input timesteps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f0f11fd-e954-49fd-9187-9d7ad5e1565c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained checkpoint from: https://huggingface.co/tungnd/climax/resolve/main/5.625deg.ckpt\n",
      "Removing key net.head.0.weight from pretrained checkpoint.\n",
      "Removing key net.head.0.bias from pretrained checkpoint.\n",
      "Removing key net.head.2.weight from pretrained checkpoint.\n",
      "Removing key net.head.2.bias from pretrained checkpoint.\n",
      "Removing key net.head.4.weight from pretrained checkpoint.\n",
      "Removing key net.head.4.bias from pretrained checkpoint.\n",
      "Removing key net.token_embeds.27.proj.weight from pretrained checkpoint\n",
      "Removing key net.token_embeds.27.proj.bias from pretrained checkpoint\n",
      "Removing key net.token_embeds.28.proj.weight from pretrained checkpoint\n",
      "Removing key net.token_embeds.28.proj.bias from pretrained checkpoint\n",
      "Removing key net.token_embeds.29.proj.weight from pretrained checkpoint\n",
      "Removing key net.token_embeds.29.proj.bias from pretrained checkpoint\n",
      "Removing key net.token_embeds.30.proj.weight from pretrained checkpoint\n",
      "Removing key net.token_embeds.30.proj.bias from pretrained checkpoint\n",
      "Removing key net.token_embeds.31.proj.weight from pretrained checkpoint\n",
      "Removing key net.token_embeds.31.proj.bias from pretrained checkpoint\n",
      "Removing key net.token_embeds.32.proj.weight from pretrained checkpoint\n",
      "Removing key net.token_embeds.32.proj.bias from pretrained checkpoint\n",
      "Removing key net.token_embeds.33.proj.weight from pretrained checkpoint\n",
      "Removing key net.token_embeds.33.proj.bias from pretrained checkpoint\n",
      "Removing key net.token_embeds.34.proj.weight from pretrained checkpoint\n",
      "Removing key net.token_embeds.34.proj.bias from pretrained checkpoint\n",
      "Removing key net.token_embeds.35.proj.weight from pretrained checkpoint\n",
      "Removing key net.token_embeds.35.proj.bias from pretrained checkpoint\n",
      "Removing key net.token_embeds.36.proj.weight from pretrained checkpoint\n",
      "Removing key net.token_embeds.36.proj.bias from pretrained checkpoint\n",
      "Removing key net.token_embeds.37.proj.weight from pretrained checkpoint\n",
      "Removing key net.token_embeds.37.proj.bias from pretrained checkpoint\n",
      "Removing key net.token_embeds.38.proj.weight from pretrained checkpoint\n",
      "Removing key net.token_embeds.38.proj.bias from pretrained checkpoint\n",
      "Removing key net.token_embeds.39.proj.weight from pretrained checkpoint\n",
      "Removing key net.token_embeds.39.proj.bias from pretrained checkpoint\n",
      "Removing key net.token_embeds.40.proj.weight from pretrained checkpoint\n",
      "Removing key net.token_embeds.40.proj.bias from pretrained checkpoint\n",
      "Removing key net.token_embeds.41.proj.weight from pretrained checkpoint\n",
      "Removing key net.token_embeds.41.proj.bias from pretrained checkpoint\n",
      "Removing key net.token_embeds.42.proj.weight from pretrained checkpoint\n",
      "Removing key net.token_embeds.42.proj.bias from pretrained checkpoint\n",
      "Removing key net.token_embeds.43.proj.weight from pretrained checkpoint\n",
      "Removing key net.token_embeds.43.proj.bias from pretrained checkpoint\n",
      "Removing key net.token_embeds.44.proj.weight from pretrained checkpoint\n",
      "Removing key net.token_embeds.44.proj.bias from pretrained checkpoint\n",
      "Removing key net.token_embeds.45.proj.weight from pretrained checkpoint\n",
      "Removing key net.token_embeds.45.proj.bias from pretrained checkpoint\n",
      "Removing key net.token_embeds.46.proj.weight from pretrained checkpoint\n",
      "Removing key net.token_embeds.46.proj.bias from pretrained checkpoint\n",
      "Removing key net.token_embeds.47.proj.weight from pretrained checkpoint\n",
      "Removing key net.token_embeds.47.proj.bias from pretrained checkpoint\n",
      "Removing key net.var_embed from pretrained checkpoint\n",
      "_IncompatibleKeys(missing_keys=['net.var_embed', 'net.time_pos_embed', 'net.time_query', 'net.head.0.weight', 'net.head.0.bias', 'net.head.2.weight', 'net.head.2.bias', 'net.head.4.weight', 'net.head.4.bias', 'net.time_agg.in_proj_weight', 'net.time_agg.in_proj_bias', 'net.time_agg.out_proj.weight', 'net.time_agg.out_proj.bias'], unexpected_keys=[])\n"
     ]
    }
   ],
   "source": [
    "# define module\n",
    "model = RainForecastModule(net, lat2d, datamodule.normalizer, pretrained_path=hparams['load'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8778a6c4-2813-4c23-add8-302f78e383c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    accelerator='gpu',\n",
    "    devices=hparams['gpus'],\n",
    "    logger=logger,\n",
    "    max_epochs=hparams['max_epochs'],\n",
    "    precision=16 if hparams['use_amp'] else 32,\n",
    "    default_root_dir=hparams['log_path'],\n",
    "    strategy=hparams['strategy'],\n",
    "    callbacks=[\n",
    "        EarlyStopping('val/val_loss', patience=5), \n",
    "        LearningRateMonitor(logging_interval='step'),\n",
    "        ModelCheckpoint(\n",
    "            dirpath='{}/{}/'.format(hparams['log_path'], hparams['version']),\n",
    "            filename='epoch-{epoch:03d}',\n",
    "            monitor='val/val_loss',\n",
    "            save_top_k=1,\n",
    "            mode='min',\n",
    "            save_last=True,\n",
    "            verbose=False,\n",
    "            auto_insert_metric_name=False,\n",
    "        )\n",
    "    ],\n",
    "    accumulate_grad_batches=hparams['acc_grad'],\n",
    ")\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984926b4-b2df-477a-af44-ab6cae496f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918450ac-f1e6-43db-ac75-9635e77e1c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model.cuda(), ckpt_path='best')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
